{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "from sklearn import ParameterGrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sepration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('input.csv')\n",
    "df_data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function the create new set every run\n",
    "def data_shufle(df_data):\n",
    "    \n",
    "    # Create the dataframe\n",
    "    df_data = pd.DataFrame(data)\n",
    "    \n",
    "    # Shuffle the data and drop the NSP column\n",
    "    data = sk.utils.shuffle(data)\n",
    "    \n",
    "    # Split the data\n",
    "    x_train, x_temp, y_train, y_temp = train_test_split(df_data, data[\"NSP\"], test_size=0.5)\n",
    "    x_validation, x_test, y_validation, y_test = train_test_split(x_temp, y_temp, test_size=0.5)\n",
    "\n",
    "\n",
    "    return x_train, x_validation, x_test, y_train, y_validation, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "def KNN_model(x_train, x_validation, y_train, y_validation):\n",
    "    \n",
    "    best_rmse = float('inf')\n",
    "    \n",
    "    # Find the best configuration\n",
    "    for k in range(1, 50):\n",
    "        for i in range(\"uniform\", \"distance\"):\n",
    "            knn_inst = KNeighborsRegressor(n_neighbors=k, weights=i)\n",
    "            knn_inst.fit(x_train, y_train)\n",
    "            knn_validation = knn_inst.predict(x_validation)\n",
    "            mse = sk.metrics.mean_squared_error(y_validation, knn_validation)\n",
    "            rmse = np.sqrt(mse)\n",
    "            \n",
    "            # See if is the best model\n",
    "            if(rmse < best_rmse):\n",
    "                best_rmse = rmse\n",
    "                best_k = k\n",
    "                best_dist = i\n",
    "                best_KNN = knn_inst\n",
    "                \n",
    "    return best_KNN, best_k, best_dist\n",
    "\n",
    "\n",
    "def KNN(x_train, x_validation, x_test, y_train, y_validation, y_test):\n",
    "    \n",
    "    # Create the model\n",
    "    best_KNN, best_k, best_dist = KNN_model(x_train, x_validation, y_train, y_validation)\n",
    "    \n",
    "    # Save the parameters\n",
    "    best_params = {\"k\": best_k, \"dist\": best_dist}\n",
    "    \n",
    "    # Predict the test data\n",
    "    knn_test = best_KNN.predict(x_test)\n",
    "    \n",
    "    # Calculate the error\n",
    "    mse = sk.metrics.mean_squared_error(y_test, knn_test)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    return best_KNN, best_params, rmse\n",
    "    \n",
    "    \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "def SVM_model(x_train, x_validation, y_train, y_validation):\n",
    "    \n",
    "    best_rmse = float('inf')\n",
    "    \n",
    "    # State the parameters\n",
    "    parameters_grid = { \"kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"], \n",
    "                        \"C\": [0.1, 1, 10, 100, 1000]}\n",
    "\n",
    "    \n",
    "    # Find the best configuration\n",
    "    for i in ParameterGrid(parameters_grid):\n",
    "        svm_inst = SVR(**i)\n",
    "        svm_inst.fit(x_train, y_train)\n",
    "        svm_validation = svm_inst.predict(x_validation)\n",
    "        mse = sk.metrics.mean_squared_error(y_validation, svm_validation)\n",
    "        rmse = np.sqrt(mse)\n",
    "        \n",
    "        # See if is the best model\n",
    "        if(rmse < best_rmse):\n",
    "            best_rmse = rmse\n",
    "            best_params = i\n",
    "            best_SVM = svm_inst\n",
    "            \n",
    "    return best_SVM, best_params\n",
    "\n",
    "def SVM(x_train, x_validation, x_test, y_train, y_validation, y_test):\n",
    "    \n",
    "    # Create the model\n",
    "    best_SVM, best_params = SVM_model(x_train, x_validation, y_train, y_validation)\n",
    "    \n",
    "    # Predict the test data\n",
    "    svm_test = best_SVM.predict(x_test)\n",
    "    \n",
    "    # Calculate the error\n",
    "    mse = sk.metrics.mean_squared_error(y_test, svm_test)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    return best_SVM, best_params, rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "def MLP_model(x_train, x_validation, y_train, y_validation):\n",
    "    \n",
    "    best_rmse = float('inf')\n",
    "    \n",
    "    # Parameters to be used\n",
    "    parameters_grid = { \"hidden_layer_sizes\": [(100, 50, 25), (21, 14, 1), (100, 100, 100)],\n",
    "                        \"activation\": [\"identity\", \"logistic\", \"tanh\", \"relu\"],\n",
    "                        \"max_iter\": [1000, 2000],\n",
    "                        \"learning_rate\": [\"constant\", \"invscaling\", \"adaptive\"]\n",
    "    }\n",
    "    \n",
    "    # Find the best configuration\n",
    "    for i in ParameterGrid(parameters_grid):\n",
    "        mlp_inst = MLPRegressor(**i)\n",
    "        mlp_inst.fit(x_train, y_train)\n",
    "        mlp_validation = mlp_inst.predict(x_validation)\n",
    "        mse = sk.metrics.mean_squared_error(y_validation, mlp_validation)\n",
    "        rmse = np.sqrt(mse)\n",
    "        \n",
    "        # Save the best model\n",
    "        if (rmse < best_rmse):\n",
    "            best_rmse = rmse\n",
    "            best_params = i\n",
    "            best_MLP = mlp_inst\n",
    "        \n",
    "    return best_MLP, best_params\n",
    "\n",
    "\n",
    "def MLP(x_train, x_validation, x_test, y_train, y_validation, y_test):\n",
    "    \n",
    "    # Create the model\n",
    "    best_MLP, best_params = MLP_model(x_train, x_validation, y_train, y_validation)\n",
    "    \n",
    "    # Predict the data\n",
    "    mlp_test = best_MLP.predict(x_test)\n",
    "    \n",
    "    # Calculate the error\n",
    "    mse = sk.metrics.mean_squared_error(y_test, mlp_test)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    return best_MLP, best_params, rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def RF_model(x_train, x_validation, y_train, y_validation):\n",
    "    \n",
    "    best_rmse = float('inf')\n",
    "    \n",
    "    # Parameters to be used\n",
    "    parameters_grid = { \"hidden_layer_sizes\": [(100, 50, 25), (21, 14, 1), (100, 100, 100)],\n",
    "                        \"activation\": [\"identity\", \"logistic\", \"tanh\", \"relu\"],\n",
    "                        \"max_iter\": [1000, 2000],\n",
    "                        \"learning_rate\": [\"constant\", \"invscaling\", \"adaptive\"]\n",
    "    }\n",
    "    \n",
    "    # Find the best config\n",
    "    for i in ParameterGrid(parameters_grid):\n",
    "        rf_inst = RandomForestRegressor(**i)\n",
    "        rf_inst.fit(x_train, y_train)\n",
    "        rf_validation = rf_inst.predict(x_validation)\n",
    "        mse = sk.metrics.mean_squared_error(y_validation, rf_validation)\n",
    "        rmse = np.sqrt(mse)\n",
    "        \n",
    "        # Save the best model\n",
    "        if rmse < best_rmse:\n",
    "            best_rmse = rmse\n",
    "            best_model = rf_inst\n",
    "            best_params = i\n",
    "            \n",
    "    return best_model, best_params\n",
    "\n",
    "def RF(x_train, x_validation, x_test, y_train, y_validation, y_test):\n",
    "    \n",
    "    # Create the model\n",
    "    best_RF, best_params = RF_model(x_train, x_validation, y_train, y_validation)\n",
    "    \n",
    "    # Predict the data\n",
    "    rf_test = best_RF.predict(x_test)\n",
    "    \n",
    "    # Calculate the error\n",
    "    mse = sk.metrics.mean_squared_error(y_test, rf_test)\n",
    "    rmse = np.sqrt(mse)   \n",
    "    \n",
    "    return best_RF, best_params, rmse  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "def GB_model(x_train, x_validation, y_train, y_validation):\n",
    "    \n",
    "    best_rmse = float('inf')\n",
    "    \n",
    "    # Parameters to be used\n",
    "    parameters_grid = { \"n_estimators\": [10, 50, 100],\n",
    "                        \"loss\": [\"squared_error\", \"quantile\", \"absolute_error\", \"huber\"],\n",
    "                        \"max_depth\": [None, 10, 50], \n",
    "                        \"learning_rate\": [0.1, 0.5, 1.0],\n",
    "                        \"min_samples_split\": [2, 5, 10], \n",
    "                        \"min_samples_leaf\": [1, 2, 4],\n",
    "    }\n",
    "    \n",
    "    # Find the best config\n",
    "    for i in ParameterGrid(parameters_grid):\n",
    "        gb_inst = GradientBoostingRegressor(**i)\n",
    "        gb_inst.fit(x_train, y_train)\n",
    "        gb_validation = gb_inst.predict(x_validation)\n",
    "        mse = sk.metrics.mean_squared_error(y_validation, gb_validation)\n",
    "        rmse = np.sqrt(mse)\n",
    "        \n",
    "        # Save the best model\n",
    "        if rmse < best_rmse:\n",
    "            best_rmse = rmse\n",
    "            best_model = gb_inst\n",
    "            best_params = i\n",
    "            \n",
    "    return best_model, best_params\n",
    "\n",
    "def GB(x_train, x_validation, x_test, y_train, y_validation, y_test):\n",
    "        \n",
    "        # Create the model\n",
    "        best_GB, best_params = GB_model(x_train, x_validation, y_train, y_validation)\n",
    "        \n",
    "        # Predict the data\n",
    "        gb_test = best_GB.predict(x_test)\n",
    "        \n",
    "        # Calculate the error\n",
    "        mse = sk.metrics.mean_squared_error(y_test, gb_test)\n",
    "        rmse = np.sqrt(mse)   \n",
    "        \n",
    "        return best_GB, best_params, rmse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
